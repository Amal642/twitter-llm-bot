# ğŸ¤– Twitter LLM Reply Bot (Local + OpenAI)

A Chrome / Brave extension that **automatically replies to selected threads on X (Twitter)** using a Large Language Model (LLM).

- ğŸ§  Uses **Ollama (local LLM)** via a lightweight proxy  
- â˜ï¸ Optional **OpenAI fallback**
- ğŸ›¡ï¸ Built with conservative anti-spam safeguards
- ğŸ¯ Replies **only to threads matching a configurable topic**
- ğŸš« Excludes sensitive or unwanted categories
- ğŸ§© Pure browser automation (no Twitter API)

---

## âœ¨ Features

- âœ… Detects the **thread author**
- âœ… Reads only the **root tweet** (not replies)
- âœ… Filters content by **{topic}**
- âœ… LLM-based topic classification
- âœ… Human-like reply posting
- âœ… Rate limiting & cooldowns
- âœ… Daily reply cap
- âœ… Memory of already-replied threads
- âœ… Random skips for natural behavior

---

## ğŸ§  Architecture

Chrome Content Script
â†“
Chrome Background Service Worker (MV3)
â†“
Local Node.js Proxy
â†“
LLM (Ollama / OpenAI)


> âš ï¸ A local proxy is required because Chrome MV3 service workers
> cannot reliably consume chunked HTTP responses from local LLM servers.


---

## ğŸš€ Setup Guide

### 1ï¸âƒ£ Prerequisites

- Node.js **v18+**
- Ollama installed and running (optional but recommended)
- Chrome or Brave browser

If using Ollama:

```bash
ollama pull llama3.2
ollama run llama3.2
```
---

2ï¸âƒ£ Start the Local LLM Proxy
cd proxy
node ollama-proxy.js


You should see:

âœ… LLM proxy running on http://127.0.0.1:3333


Leave this terminal open.

-----------

3ï¸âƒ£ Load the Chrome Extension

Open chrome://extensions

Enable Developer mode

Click Load unpacked

Select the extension/ folder

Reload the extension after any code change

4ï¸âƒ£ Enable the Bot

Open X (Twitter)

Click the extension popup

Enable the bot

Open a single tweet thread

The bot will evaluate the thread and decide whether to reply

ğŸ›¡ï¸ Safety & Anti-Detection

The bot includes multiple safeguards to reduce automated behavior detection:

â±ï¸ Cooldown between replies

ğŸ“† Daily reply limits

ğŸ² Random skip probability

ğŸ§  Topic classification via LLM

ğŸ§¾ Memory of previously processed threads

Defaults are intentionally conservative.

âš™ï¸ Configuration

Key configuration options live in content.js:

const DAILY_LIMIT = 5;
const MIN_COOLDOWN_MS = 10 * 60 * 1000; // 10 minutes


You can customize:

{topic} definition

Topic classification prompt

Blocked categories

Reply style and tone

Random skip probability

ğŸ” LLM Options
Local (Ollama)

Fully local

No API keys

Requires proxy due to Chrome MV3 limitations

Cloud (OpenAI)

Reliable and fast

Requires API key

Can be used as fallback

âš ï¸ Disclaimer

This project is for educational and experimental purposes only.

Automated interactions may violate platform terms

Use responsibly and at your own risk

Prefer secondary or test accounts

ğŸ§© Future Improvements

Timeline auto-scanning

Multi-thread processing

Topic confidence scoring

Multiple personas

Streaming replies

Web dashboard

Dockerized proxy

ğŸ“œ License

MIT â€” use responsibly.


---
